{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашнее задание № 9\n",
    "\n",
    "Используя датасет из https://github.com/Phylliida/Dialogue-Datasets\n",
    "Используя материалы лекции (ноутбук ) обучить модель seq2seq с механизмом внимания для получения ответов на вопросы.\n",
    "Взять 1000 сценариев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:41:36.157114Z",
     "start_time": "2019-12-01T16:41:36.153943Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZdZvVz27mE7c"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import zipfile\n",
    "import collections\n",
    "from torch import nn\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiD3rif6kVxM"
   },
   "source": [
    "Использовал датасет яндекс толоки https://toloka.yandex.ru/datasets/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2o3sL0FZwjTr",
    "outputId": "42ef9f58-ea15-4731-cd3f-e3878ab266b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZAL8WWIkV15"
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('./drive/My Drive/dialogues.tsv', sep='\\t')\n",
    "data = pd.read_csv('dialogues.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DZtxoc7yM6G"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "qyD9mbL-xojX",
    "outputId": "c99cb4c7-b2e5-4329-eacd-1a02dc85d988",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' Привет) расскажи о себе   ', ' Привет) под вкусный кофеек настроение поболтать появилось )   ', ' Что читаешь? Мне нравится классика   ', ' Я тоже люблю пообщаться   ', ' Люблю животных, просто обожаю, как и свою работу)   ', ' Я фантастику люблю   ', ' А я выращиваю фиалки   ', ' И веду здоровый и активный образ жизни!   ', ' Ух ты, интересно.   ', ' Ты случайно не принц на белом коне? Я его очень жду ..   ', ' А у меня из хобби каждую неделю тусить с моим лучшим другом)  ']\n",
      "[' ', ' Привет!   ', ' Привет,Как жизнь?   ', ' Отлично) Солнышко светит, птички поют!   ', ' Я вот сегодня понял, что меня тупо используют, всем нужны от меня лишь деньги, ненавижу людей   ', ' Чем занимаешься по жизни, я вот бизнесмен.   ', ' А я вот учу детей, работаю с начальными классами   ', ' Не все люди такие, как ты говоришь   ', ' Помимо работы чем еще ты занимаешься?   ', ' К свадьбе готовлюсь   ', ' А ты?   ', ' Вот видишь) значит, нашел такую женщину, которой не нужны от тебя деньги   ', ' Да я надеюсь на это,люблю ее  ']\n",
      "[' ', ' Привет   ', ' Как дела ?   ', ' Добрый день!   ', ' Хорошо,  чем увлекаетесь?   ', ' Я бегаю по утрам а ты?   ', ' Есть любимые вещи или еда ?   ', ' Занят ?   ', ' Я люблю петь в караоке)   ', ' Круто )   ', ' Люблю готовить пасту,  у меня классно получается!   ', ' Любишь готовить?   ', ' Это хорошо   ', ' Я не эксперт   ', ' Я люблю есть арбуз  ']\n",
      "[' ', ' Здравствуйте   ', ' Я Леша   ', ' Здравствуйте   ', ' Я Егор   ', ' Я учусь в 6 классе   ', ' А мне 30 и я уже работаю   ', ' А я тоже хочу. На машину скопить.   ', ' Правда мне не нравится моя работа   ', ' Почему?   ', ' Мало платят   ', ' На семью не хватает   ', ' Жена и трое детей   ', ' А... а я на машину...   ', ' Ого  ']\n",
      "[' ', ' Привет!   ', ' Привет!   ', ' Как твои дела?   ', ' Нормально, готовлюсь ко сну. Завтра снова в школу . Не люблю учиться.   ', ' А твои как?   ', ' Всё хорошо,спать не хочется,думаю фильм посмотреть   ', ' Какой фильм?   ', ' Ещё не решила, может быть детектив какой нибудь. А как в школе у тебя?   ', ' Хорошо, ещё учусь, но скоро закончу. Уже мечтаю работать , а не сидеть за учебниками. А ты работаешь или учишься ещё?   ', ' А я работаю, мне нравится моя работа, кем планируешь работать?   ', ' Хочу быть психологом. А кем ты работаешь?   ', ' Не поверишь....я психолог   ', ' Круто! И как тебе?   ', ' Мне нравится,я люблю свою работу.   ', ' Это классно. Хочу также. А ты одна живешь?   ', ' Нет,я живу с мамой,а ты?   ', ' Я тоже с родителями живу, обожаю их. Но иногда хочется пожить одной.   ', ' Мне тоже, я люблю быть дома одна, мне эио часто удаётся , мы с мамой работаем в разное время, а в каком городе ты живёшь?   ', ' А у меня мама домохозяйка, поэтому редко бываю дома одна. В Питере, а ты?   ', ' А я в Ростове на Дону.   ', ' Хочу там побывать.   ', ' Ладно, мне пора ложиться   ', ' Рада была поболтать!   ', ' Спокойной ночи   ', ' Поздно уже может до завтра? СПОКОЙНОЙ НОЧИ, ПРИЕЗЖАЙ В ГОСТИ  ']\n"
     ]
    }
   ],
   "source": [
    "for i in data['dialogue'].head():\n",
    "    print(re.split(r'Пользователь \\d:', re.sub('<..*?>', ' ' , i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNIKlTrMzszq"
   },
   "outputs": [],
   "source": [
    "data['dialogue_list']= data['dialogue'].apply(lambda x: re.split(r'Пользователь ', re.sub('\\s\\s+',' ',re.sub('<..*?>', ' ' , x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "wHPH3mEQAu3T",
    "outputId": "99450f6e-5a61-41d3-9074-a46eedef809b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '2: Привет) расскажи о себе ', '1: Привет) под вкусный кофеек настроение поболтать появилось ) ', '2: Что читаешь? Мне нравится классика ', '2: Я тоже люблю пообщаться ', '1: Люблю животных, просто обожаю, как и свою работу) ', '1: Я фантастику люблю ', '2: А я выращиваю фиалки ', '2: И веду здоровый и активный образ жизни! ', '1: Ух ты, интересно. ', '2: Ты случайно не принц на белом коне? Я его очень жду .. ', '1: А у меня из хобби каждую неделю тусить с моим лучшим другом) ']\n",
      "[' ', '1: Привет! ', '2: Привет,Как жизнь? ', '1: Отлично) Солнышко светит, птички поют! ', '2: Я вот сегодня понял, что меня тупо используют, всем нужны от меня лишь деньги, ненавижу людей ', '2: Чем занимаешься по жизни, я вот бизнесмен. ', '1: А я вот учу детей, работаю с начальными классами ', '1: Не все люди такие, как ты говоришь ', '1: Помимо работы чем еще ты занимаешься? ', '2: К свадьбе готовлюсь ', '2: А ты? ', '1: Вот видишь) значит, нашел такую женщину, которой не нужны от тебя деньги ', '2: Да я надеюсь на это,люблю ее ']\n",
      "[' ', '1: Привет ', '1: Как дела ? ', '2: Добрый день! ', '2: Хорошо, чем увлекаетесь? ', '1: Я бегаю по утрам а ты? ', '1: Есть любимые вещи или еда ? ', '1: Занят ? ', '2: Я люблю петь в караоке) ', '1: Круто ) ', '2: Люблю готовить пасту, у меня классно получается! ', '2: Любишь готовить? ', '1: Это хорошо ', '1: Я не эксперт ', '1: Я люблю есть арбуз ']\n",
      "[' ', '2: Здравствуйте ', '2: Я Леша ', '1: Здравствуйте ', '1: Я Егор ', '2: Я учусь в 6 классе ', '1: А мне 30 и я уже работаю ', '2: А я тоже хочу. На машину скопить. ', '1: Правда мне не нравится моя работа ', '2: Почему? ', '1: Мало платят ', '1: На семью не хватает ', '1: Жена и трое детей ', '2: А... а я на машину... ', '2: Ого ']\n",
      "[' ', '1: Привет! ', '2: Привет! ', '2: Как твои дела? ', '1: Нормально, готовлюсь ко сну. Завтра снова в школу . Не люблю учиться. ', '1: А твои как? ', '2: Всё хорошо,спать не хочется,думаю фильм посмотреть ', '1: Какой фильм? ', '2: Ещё не решила, может быть детектив какой нибудь. А как в школе у тебя? ', '1: Хорошо, ещё учусь, но скоро закончу. Уже мечтаю работать , а не сидеть за учебниками. А ты работаешь или учишься ещё? ', '2: А я работаю, мне нравится моя работа, кем планируешь работать? ', '1: Хочу быть психологом. А кем ты работаешь? ', '2: Не поверишь....я психолог ', '1: Круто! И как тебе? ', '2: Мне нравится,я люблю свою работу. ', '1: Это классно. Хочу также. А ты одна живешь? ', '2: Нет,я живу с мамой,а ты? ', '1: Я тоже с родителями живу, обожаю их. Но иногда хочется пожить одной. ', '2: Мне тоже, я люблю быть дома одна, мне эио часто удаётся , мы с мамой работаем в разное время, а в каком городе ты живёшь? ', '1: А у меня мама домохозяйка, поэтому редко бываю дома одна. В Питере, а ты? ', '2: А я в Ростове на Дону. ', '1: Хочу там побывать. ', '1: Ладно, мне пора ложиться ', '1: Рада была поболтать! ', '1: Спокойной ночи ', '2: Поздно уже может до завтра? СПОКОЙНОЙ НОЧИ, ПРИЕЗЖАЙ В ГОСТИ ']\n"
     ]
    }
   ],
   "source": [
    "for i in data['dialogue_list'].head():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSlq41Bb0Eu6"
   },
   "outputs": [],
   "source": [
    "data['dialogue_list'] = data['dialogue_list'].apply(lambda x: list(filter(lambda s: s.strip() != '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "p4xNkjZc0mtC",
    "outputId": "51007680-aa6f-4ddb-b2f1-04c5706c103b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1: Привет! ',\n",
       " '2: Привет,Как жизнь? ',\n",
       " '1: Отлично) Солнышко светит, птички поют! ',\n",
       " '2: Я вот сегодня понял, что меня тупо используют, всем нужны от меня лишь деньги, ненавижу людей ',\n",
       " '2: Чем занимаешься по жизни, я вот бизнесмен. ',\n",
       " '1: А я вот учу детей, работаю с начальными классами ',\n",
       " '1: Не все люди такие, как ты говоришь ',\n",
       " '1: Помимо работы чем еще ты занимаешься? ',\n",
       " '2: К свадьбе готовлюсь ',\n",
       " '2: А ты? ',\n",
       " '1: Вот видишь) значит, нашел такую женщину, которой не нужны от тебя деньги ',\n",
       " '2: Да я надеюсь на это,люблю ее ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['dialogue_list'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAao2hpB1Gk9"
   },
   "outputs": [],
   "source": [
    "def add_seq_pers(x):\n",
    "    cur_pers = ''\n",
    "    new_list = []\n",
    "    cur_str = ''\n",
    "    for i in x:\n",
    "        if i[:2] != cur_pers:\n",
    "          #                              смайлики                       двойные пробелы .       убирает два знака типа ? .              ставит пробел между словом и знаком\n",
    "            new_list.append(re.sub(r'\\d+','',re.sub(r'[\\?\\!\\:\\)\\(\\-]{2,5}','',re.sub(r'\\s\\s+',' ', re.sub(r'([\\!\\.?,\\)\\(\\:\\-)])\\s*([\\!\\.?,\\)\\(\\:\\-)])+','.',re.sub(r'([\\!\\.?,\\)\\(\\:\\-)]+)', r' \\1',re.sub(r'[\\)\\()]','.', cur_str.lower()).strip()))))))\n",
    "            cur_str = i[2:]\n",
    "            cur_pers = i[:2]    \n",
    "        else: \n",
    "            cur_str += ' . ' +  i[2:] \n",
    "    return new_list[1:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWVW_Rn08jm3"
   },
   "outputs": [],
   "source": [
    "data['dialogue_list'] = data['dialogue_list'].apply(add_seq_pers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "OWslVEEJ8rFA",
    "outputId": "c680d280-bb3a-42c5-c6b5-5b28901f7ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['привет . расскажи о себе', 'привет . под вкусный кофеек настроение поболтать появилось .', 'что читаешь ? мне нравится классика . я тоже люблю пообщаться', 'люблю животных , просто обожаю , как и свою работу . я фантастику люблю', 'а я выращиваю фиалки . и веду здоровый и активный образ жизни !', 'ух ты , интересно .', 'ты случайно не принц на белом коне ? я его очень жду .']\n",
      "['привет !', 'привет ,как жизнь ?', 'отлично . солнышко светит , птички поют !', 'я вот сегодня понял , что меня тупо используют , всем нужны от меня лишь деньги , ненавижу людей . чем занимаешься по жизни , я вот бизнесмен .', 'а я вот учу детей , работаю с начальными классами . не все люди такие , как ты говоришь . помимо работы чем еще ты занимаешься ?', 'к свадьбе готовлюсь . а ты ?', 'вот видишь . значит , нашел такую женщину , которой не нужны от тебя деньги']\n",
      "['привет . как дела ?', 'добрый день . хорошо , чем увлекаетесь ?', 'я бегаю по утрам а ты . есть любимые вещи или еда . занят ?', 'я люблю петь в караоке .', 'круто .', 'люблю готовить пасту , у меня классно получается . любишь готовить ?']\n",
      "['здравствуйте . я леша', 'здравствуйте . я егор', 'я учусь в  классе', 'а мне  и я уже работаю', 'а я тоже хочу . на машину скопить .', 'правда мне не нравится моя работа', 'почему ?', 'мало платят . на семью не хватает . жена и трое детей']\n",
      "['привет !', 'привет . как твои дела ?', 'нормально , готовлюсь ко сну . завтра снова в школу . не люблю учиться . а твои как ?', 'всё хорошо ,спать не хочется ,думаю фильм посмотреть', 'какой фильм ?', 'ещё не решила , может быть детектив какой нибудь . а как в школе у тебя ?', 'хорошо , ещё учусь , но скоро закончу . уже мечтаю работать , а не сидеть за учебниками . а ты работаешь или учишься ещё ?', 'а я работаю , мне нравится моя работа , кем планируешь работать ?', 'хочу быть психологом . а кем ты работаешь ?', 'не поверишь .я психолог', 'круто ! и как тебе ?', 'мне нравится ,я люблю свою работу .', 'это классно . хочу также . а ты одна живешь ?', 'нет ,я живу с мамой ,а ты ?', 'я тоже с родителями живу , обожаю их . но иногда хочется пожить одной .', 'мне тоже , я люблю быть дома одна , мне эио часто удаётся , мы с мамой работаем в разное время , а в каком городе ты живёшь ?', 'а у меня мама домохозяйка , поэтому редко бываю дома одна . в питере , а ты ?', 'а я в ростове на дону .', 'хочу там побывать . ладно , мне пора ложиться . рада была поболтать . спокойной ночи']\n",
      "['привет !', 'привет . как дела .', 'у меня все замечательно . а твои дела как ? чем занимаешься по жизни . ?', 'рада новому знакомству ! у меня тоже всё неплохо . недавно нашла работу по специальности . я дизайнер . а ты чем занимаешься . ты откуда ?', 'я продавец , выращиванию овощи и фрукты на даче и продаю их . я ещё кстати и дачница . я с украины . а ты откуда ?', 'круто ! я из беларуси , минск', 'я продавец , выращиваю овощи и фрукты на даче и продаю их . я ещё кстати и дачница . как зовут тебя ?', 'а я мечтаю купить дачу , но пока не могу себе этого позволить . катя , а тебя ?', 'ира ,приятно познакомится . а я мечтаю жить возле моря .', 'кстати , была в украине , мне там очень понравилось . я вообще люблю путешествовать', 'обожаю море .', 'приятно познакомиться', 'в каком городе украины ты была ?', 'о , я тоже люблю море . . много где , и в крыму .тогда это была ещё украина .', 'жду не дождусь лета , что бы поехать на море 🌊 . к стати , ты любишь собак ?', 'мы путешествовали с друзьями на машине , поэтому много где побывали . хотя я больше люблю пешие прогулки .', 'пешие прогулки это здорово .', 'о да , я люблю животных . но я больше кошатница . у меня кот вегас и черепаха матильда . а у тебя есть собака ?', 'у меня аж две собаки .', 'здорово ! как их зовут ?', 'аза и герда , две девочки дворняжки . подобрала их на улице , кто бросил совсем маленьких , пришлось о них много заботиться . подобрала их на улице , кто - то бросил совсем маленьких , пришлось о них много заботиться .', 'я тоже . планирую новое путешествие летом . я уже побывала в  странах . а ты любишь путешествовать . ты молодец , спасла две жизни . это здорово', 'пока что не удавалось побывать в других странах , но на это лето грандиозные планы .', 'какие ?', 'поехать сначала в германию , а потом в россию к родственникам', 'кстати , когда я была во львове , ходила на концерт океан эльзи . очень люблю эту группу и вообще рок . а какую музыку ты слушаешь ?', 'а я рок не люблю , мне нравиться джаз . а вообще часто слушаю поп и рэп', 'я была в германии , мне там очень понравилось , особенно красивые замки . ну из джаза родился рок .']\n",
      "['привет', 'привет', 'как дела ? я и норвегии ,а ты ?', 'хорошо , а у тебя как ? я из россии . кто по профессии ты ?', 'тоже хорошо , чего не спишь ?', 'расскажи о себе . стираю белье , жду .', 'я не работаю .', 'а почему ?', 'живу в норвегии с  лет , сейчас мне  , уехали с родителями , отцу надо было по работе , остались . периодически бываем в россии', 'круто . я бы тоже поехал . когда последний раз был в россии ?', 'летом  . давно . скучаю по россии . ты из какого города', 'я из тулы .', 'чем увлекаешься ?', 'люблю лыжи . а ты чем увлекаешься ?', 'я тоже', 'а ты чем занимаешься , если не работаешь ?', 'лошадей люблю , у нас были давно ,  лошадей . вот следил за ними ,отцу помогал . с тех пор так , подрабатываю иногда', 'как с трудоустройством в норвегии ?', 'нормально . безработных не очень много . думаю это от неопределенности .', 'люблю путешествовать , думаю посетить страну']\n",
      "['привет', 'привет ! как дела .', 'ой , слушай , довольно таки неплохо . гораздо лучше , чем днём . а твои как ?', 'да тоже неплохо . вот только с рыбалки вернулся ! люблю это дело , расслабляет и помогает отвлечься от ежедневной рутины . а ты чем любишь заниматься в свободное время ?', 'в свободное от работы время я люблю готовиться всякие вкусности . в детстве мама меня учила готовить , и тем самым я полюбила это дело . а самое удобное в том , что ты хорошо готовишь , это то , что я очень люблю сложное и могу взять и , например , испечь себе что - нибудь . . в свободное от работы время я люблю готовить всякие вкусности . в детстве мама меня учила готовить , и тем самым я полюбила это дело . а самое удобное в том , что ты хорошо готовишь , это то , что я очень люблю сложное и могу взять и , например , испечь себе что - нибудь . . в свободное от работы время я люблю готовить всякие вкусности . в детстве мама меня учила готовить , и тем самым я полюбила это дело . а самое удобное в том , что ты хорошо готовишь , это то , что я очень люблю сладкое и могу взять и , например , испечь себе что - нибудь . . а что ты любишь больше всего из еды . и да , рыбалка это тоже офигенно . особенно не с берега , а на лодочке . ух . прекрасно', 'ух ты ! готовка - это здорово ! я вот в этом не особо силён . жена всегда у нас главной по кухне была , дочку даже учила потихоньку . но в прошлом году её , к несчастью , не стало . теперь вот с дочкой вдвоём периодически пытаемся что - то кошеварить , но так вкусно , как у неё , конечно , не получается . у нас любимое блюдо было паста с креветками . можно сказать , фирменное .', 'ох , соболезную .', 'на работе правда парни буржуем всегда звали за это . говорили типа автослесарю то гоже картошку жареную есть , а не пасту . . спасибо 🙏🏻 . а ты музыку любишь ? играешь на чём - нибудь может быть .', 'я вот не замужем , и никогда не была . живу со своими любимыми собачками . обожаю просто собак , они офигенные . вот ты любишь собак ? или может других каких животных . нет , я ни на чём не играю . некогда . я ж работаю , врач я , жизни спасаю .', 'я собак тоже очень люблю , а вот дочка все котёнка выпрашивает .', 'сколько дочке лет . вообще , это грех . приобрети для нее котенка . это ж здорово . и щеночка можно ещё ; .', 'о , это очень здорово . ответственная работа очень . а я вот на гитаре люблю поиграть , когда свободная минутка выдаётся . самоучка , конечно , но что - то вроде получается .  вот будет на следующей неделе . думаю , и правда на день рождения порадовать ее пушистым подарком . уж очень просит .', 'ну , у меня друг играет , тоже самоучка , но кааак он играет .😍😍😍 . дааа , подари , она будет очень счастлива .', 'спасибо за совет . думаю , воспользуюсь им 👍🏻🙂', 'ладно , доброй ночи . спасибо за компанию']\n",
      "['привет .', 'здрорво ! ты где территориально ?', 'украина', 'огого . я тоже за границей . в австрилии живу . какие увлечения ?', 'поняла но я домработница редко где бываю . люблю варить кофе . и пить . раскажи о себе', 'ого , круто . домработницей , наверное , тяжело работать . вот у меня легко все . работаю в саду только . увлекаюсь ботаникой . сажаю разные виды деревьев . а ты любишь растения ?', 'да люблю ,а также свою семю но я бездетна', 'кстати , вывожу новые сорта . вот уже семь новых сортов ялонь посадила . ну , думаю , что современная медицина реашет почти все проблемы . ты замужем ?']\n",
      "['привет из норвегии !', 'приветик , ничего себе . . там я ещё не была', 'у нас очень красиво , мой попугай всегда говорит : красотаааа . . у вас есть животные ?', 'ахах . а девушкам он такое говорит интересно . я* скромно* считаю себя довольно красивой . . у меня была собачка пуговка . сейчас я не готова нового питомца завести .', 'с детства не люблю деревья , я с них часто падал , поэтому после университета , я вырубил большую поляну возля дома и завел десять лошадей . кстати , тогда я бросил курить , чтоб лошади не пострадали . . у вас есть питомцы ?', 'класс , обожаю животных . ты живёшь в частном доме ? я в многоэтажке на первом , так что никакой поляны своей . здорово , что ты о них заботишься . а у тебя есть белые лошади ?', 'уже нет никаких , пришлось вывести их . . чем увлекаетесь ?', 'чтооо . а я мечтала о белой лошади с белой гривой , я бы офигенно на ней смотрелась , только представь красивая блондинка с отличной фигурой . . я кстати не в отношениях . увлекаюсь нуу люблю читать глянец , а так учёба все . а ты ?', 'я книги люблю . извините пора бежать , дочь из сада забирать']\n"
     ]
    }
   ],
   "source": [
    "for i in data['dialogue_list'].head(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:48.938198Z",
     "start_time": "2019-12-01T16:33:48.546080Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YXKXBkJtmE76"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:49.094086Z",
     "start_time": "2019-12-01T16:33:48.940601Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "xsiDp1nimE7-",
    "outputId": "890ff673-2983-4fd2-81aa-60ac3dfbcb7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAARUUlEQVR4nO3dX4zdZZ3H8ffHtlB2BYEy25AO7tTYxFSzIjZQoxcuRChghAs0ELM0prEXYoKJiVt2kyX+IYEbURIlS6ShGNfKuhoawO12C2azF0AHQaCwLCNCmAZobQusMSDF716cp/VQZjpnysyctuf9Sk7O8/s+z/md5zyBfub358ykqpAkDbZ39XsCkqT+MwwkSYaBJMkwkCRhGEiSgPn9nsDhOu2002pkZKTf05Cko8ZDDz30u6oamqjvqA2DkZERRkdH+z0NSTpqJHlusj5PE0mSDANJkmEgSeIovmYgSYPsjTfeYHx8nNdee+1tfQsXLmR4eJgFCxb0vD/DQJKOQuPj45x44omMjIyQ5EC9qti9ezfj4+MsXbq05/15mkiSjkKvvfYaixYteksQACRh0aJFEx4xHIphIElHqYODYKr6oRgGkiTDQJI0oBeQR9bdfaD97PUX93EmknT4qmrCU0KH80fLPDKQpKPQwoUL2b1799v+4d9/N9HChQuntb+BPDKQpKPd8PAw4+Pj7Nq16219+79nMB2GgSQdhRYsWDCt7xFMxdNEkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn0GAZJnk3yWJJHkoy22qlJtiR5uj2f0upJclOSsSSPJjmraz+r2/ink6zuqn+07X+svXb6f7NNknTYpnNk8LdVdWZVrWjb64CtVbUM2Nq2AS4ElrXHWuBm6IQHcC1wDnA2cO3+AGljvtj1ulWH/YkkSdP2Tk4TXQJsaO0NwKVd9dur437g5CSnAxcAW6pqT1XtBbYAq1rfSVV1f3X+SsPtXfuSJM2BXsOggP9I8lCSta22uKpeaO0XgcWtvQR4vuu14612qPr4BHVJ0hzp9Y/bfKKqdiT5K2BLkv/p7qyqSjL9P7o5TS2I1gK8973vne23k6SB0dORQVXtaM87gZ/TOef/UjvFQ3ve2YbvAM7oevlwqx2qPjxBfaJ53FJVK6pqxdDQUC9TlyT1YMowSPKXSU7c3wbOBx4HNgH77whaDdzZ2puAK9tdRSuBV9rppM3A+UlOaReOzwc2t75Xk6xsdxFd2bUvSdIc6OU00WLg5+1uz/nAv1TVvyfZBtyRZA3wHPC5Nv4e4CJgDPgD8AWAqtqT5JvAtjbuG1W1p7W/BNwGnAD8oj0kSXNkyjCoqmeAD09Q3w2cN0G9gKsm2dd6YP0E9VHgQz3MV5I0C/wGsiTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJaYRBknlJHk5yV9temuSBJGNJfpLkuFY/vm2Ptf6Rrn1c0+pPJbmgq76q1caSrJu5jydJ6sV0jgyuBp7s2r4BuLGq3g/sBda0+hpgb6vf2MaRZDlwOfBBYBXw/RYw84DvARcCy4Er2lhJ0hzpKQySDAMXAz9o2wHOBX7ahmwALm3tS9o2rf+8Nv4SYGNVvV5VvwXGgLPbY6yqnqmqPwIb21hJ0hzp9cjgO8DXgD+17UXAy1W1r22PA0taewnwPEDrf6WNP1A/6DWT1d8mydoko0lGd+3a1ePUJUlTmTIMknwa2FlVD83BfA6pqm6pqhVVtWJoaKjf05GkY8b8HsZ8HPhMkouAhcBJwHeBk5PMbz/9DwM72vgdwBnAeJL5wHuA3V31/bpfM1ldkjQHpjwyqKprqmq4qkboXAC+t6o+D9wHXNaGrQbubO1NbZvWf29VVatf3u42WgosAx4EtgHL2t1Jx7X32DQjn06S1JNejgwm8/fAxiTfAh4Gbm31W4EfJhkD9tD5x52q2p7kDuAJYB9wVVW9CZDky8BmYB6wvqq2v4N5SZKmaVphUFW/BH7Z2s/QuRPo4DGvAZ+d5PXXAddNUL8HuGc6c5EkzRy/gSxJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJvLNvIB8TRtbdfaD97PUX93EmktQ/HhlIkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSaKHMEiyMMmDSX6dZHuSr7f60iQPJBlL8pMkx7X68W17rPWPdO3rmlZ/KskFXfVVrTaWZN3Mf0xJ0qH0cmTwOnBuVX0YOBNYlWQlcANwY1W9H9gLrGnj1wB7W/3GNo4ky4HLgQ8Cq4DvJ5mXZB7wPeBCYDlwRRsrSZojU4ZBdfy+bS5ojwLOBX7a6huAS1v7krZN6z8vSVp9Y1W9XlW/BcaAs9tjrKqeqao/AhvbWEnSHOnpmkH7Cf4RYCewBfgN8HJV7WtDxoElrb0EeB6g9b8CLOquH/SayeoTzWNtktEko7t27epl6pKkHvQUBlX1ZlWdCQzT+Un+A7M6q8nncUtVraiqFUNDQ/2YgiQdk6Z1N1FVvQzcB3wMODnJ/NY1DOxo7R3AGQCt/z3A7u76Qa+ZrC5JmiO93E00lOTk1j4B+BTwJJ1QuKwNWw3c2dqb2jat/96qqla/vN1ttBRYBjwIbAOWtbuTjqNzkXnTTHw4SVJv5k89hNOBDe2un3cBd1TVXUmeADYm+RbwMHBrG38r8MMkY8AeOv+4U1Xbk9wBPAHsA66qqjcBknwZ2AzMA9ZX1fYZ+4SSpClNGQZV9SjwkQnqz9C5fnBw/TXgs5Ps6zrgugnq9wD39DBfSdIs8BvIkiTDQJLU2zWDgTGy7u4D7Wevv7iPM5GkueWRgSTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJ9BAGSc5Icl+SJ5JsT3J1q5+aZEuSp9vzKa2eJDclGUvyaJKzuva1uo1/OsnqrvpHkzzWXnNTkszGh5UkTayXI4N9wFerajmwErgqyXJgHbC1qpYBW9s2wIXAsvZYC9wMnfAArgXOAc4Grt0fIG3MF7tet+qdfzRJUq+mDIOqeqGqftXa/wc8CSwBLgE2tGEbgEtb+xLg9uq4Hzg5yenABcCWqtpTVXuBLcCq1ndSVd1fVQXc3rUvSdIcmNY1gyQjwEeAB4DFVfVC63oRWNzaS4Dnu1423mqHqo9PUJ/o/dcmGU0yumvXrulMXZJ0CD2HQZJ3A/8GfKWqXu3uaz/R1wzP7W2q6paqWlFVK4aGhmb77SRpYPQUBkkW0AmCH1XVz1r5pXaKh/a8s9V3AGd0vXy41Q5VH56gLkmaI73cTRTgVuDJqvp2V9cmYP8dQauBO7vqV7a7ilYCr7TTSZuB85Oc0i4cnw9sbn2vJlnZ3uvKrn1JkubA/B7GfBz4O+CxJI+02j8A1wN3JFkDPAd8rvXdA1wEjAF/AL4AUFV7knwT2NbGfaOq9rT2l4DbgBOAX7SHJGmOTBkGVfXfwGT3/Z83wfgCrppkX+uB9RPUR4EPTTUXSdLs8BvIkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJorc/ezmQRtbdfaD97PUX93EmkjT7PDKQJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJHsIgyfokO5M83lU7NcmWJE+351NaPUluSjKW5NEkZ3W9ZnUb/3SS1V31jyZ5rL3mpiSZ6Q8pSTq0Xo4MbgNWHVRbB2ytqmXA1rYNcCGwrD3WAjdDJzyAa4FzgLOBa/cHSBvzxa7XHfxefTey7u4DD0k6Fk0ZBlX1X8Ceg8qXABtaewNwaVf99uq4Hzg5yenABcCWqtpTVXuBLcCq1ndSVd1fVQXc3rUvSdIcOdxrBour6oXWfhFY3NpLgOe7xo232qHq4xPUJ5RkbZLRJKO7du06zKlLkg72ji8gt5/oawbm0st73VJVK6pqxdDQ0Fy8pSQNhMMNg5faKR7a885W3wGc0TVuuNUOVR+eoC5JmkOHGwabgP13BK0G7uyqX9nuKloJvNJOJ20Gzk9ySrtwfD6wufW9mmRlu4voyq59SZLmyJR/6SzJj4FPAqclGadzV9D1wB1J1gDPAZ9rw+8BLgLGgD8AXwCoqj1Jvglsa+O+UVX7L0p/ic4dSycAv2gPSdIcmjIMquqKSbrOm2BsAVdNsp/1wPoJ6qPAh6aahyRp9vgNZEmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEj381lK91ci6uw+0n73+4j7ORJJmjkcGkiTDQJJkGEiS8JrBO+L1A0nHCo8MJEmGgSTJMJAkYRhIkjAMJEkYBpIkvLV0xnibqaSjmUcGkiSPDGaDRwmSjjYeGUiSDANJ0hF0mijJKuC7wDzgB1V1fZ+nNCO6Txl18/SRpCPJEREGSeYB3wM+BYwD25Jsqqon+juz2eN1BUlHkiMiDICzgbGqegYgyUbgEuCYDYNukx09TMbwkDTTjpQwWAI837U9Dpxz8KAka4G1bfP3SZ46jPc6DfjdYbzuiJEbZnR3R/16zALX5K1cj7c6mtfjryfrOFLCoCdVdQtwyzvZR5LRqloxQ1M66rkeb+eavJXr8VbH6nocKXcT7QDO6NoebjVJ0hw4UsJgG7AsydIkxwGXA5v6PCdJGhhHxGmiqtqX5MvAZjq3lq6vqu2z9Hbv6DTTMcj1eDvX5K1cj7c6JtcjVdXvOUiS+uxIOU0kSeojw0CSNFhhkGRVkqeSjCVZ1+/5zIUk65PsTPJ4V+3UJFuSPN2eT2n1JLmprc+jSc7q38xnR5IzktyX5Ikk25Nc3eoDuSZJFiZ5MMmv23p8vdWXJnmgfe6ftBs7SHJ82x5r/SP9nP9sSTIvycNJ7mrbx/x6DEwYdP3KiwuB5cAVSZb3d1Zz4jZg1UG1dcDWqloGbG3b0FmbZe2xFrh5juY4l/YBX62q5cBK4Kr238GgrsnrwLlV9WHgTGBVkpXADcCNVfV+YC+wpo1fA+xt9RvbuGPR1cCTXdvH/npU1UA8gI8Bm7u2rwGu6fe85uizjwCPd20/BZze2qcDT7X2PwNXTDTuWH0Ad9L5nVgDvybAXwC/ovPt/98B81v9wP87dO74+1hrz2/j0u+5z/A6DNP5geBc4C4gg7AeA3NkwMS/8mJJn+bSb4ur6oXWfhFY3NoDtUbtkP4jwAMM8Jq0UyKPADuBLcBvgJeral8b0v2ZD6xH638FWDS3M5513wG+BvypbS9iANZjkMJAE6jOjzQDd39xkncD/wZ8pape7e4btDWpqjer6kw6PxGfDXygz1PqmySfBnZW1UP9nstcG6Qw8Fde/NlLSU4HaM87W30g1ijJAjpB8KOq+lkrD/SaAFTVy8B9dE6DnJxk/5dSuz/zgfVo/e8Bds/xVGfTx4HPJHkW2EjnVNF3GYD1GKQw8Fde/NkmYHVrr6Zz3nx//cp2B81K4JWuUyfHhCQBbgWerKpvd3UN5JokGUpycmufQOf6yZN0QuGyNuzg9di/TpcB97YjqWNCVV1TVcNVNULn34h7q+rzDMJ69PuixVw+gIuA/6VzTvQf+z2fOfrMPwZeAN6gc65zDZ1zmluBp4H/BE5tY0PnjqvfAI8BK/o9/1lYj0/QOQX0KPBIe1w0qGsC/A3wcFuPx4F/avX3AQ8CY8C/Ase3+sK2Pdb639fvzzCLa/NJ4K5BWQ9/HYUkaaBOE0mSJmEYSJIMA0mSYSBJwjCQJGEYSJIwDCRJwP8Dfxb10eUcdjUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(l.split(' ')) for l in sum(data['dialogue_list'].tolist(),[])], bins=100)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "SiJEXFoHUNr5",
    "outputId": "578af0d1-027b-4166-c80d-3e389fd57cbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(l) for l in data['dialogue_list'].tolist()], bins=100)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nXrsihvUjBE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:49.106131Z",
     "start_time": "2019-12-01T16:33:49.096232Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "m6eHX9DcmE8B"
   },
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[0])\n",
    "        self.token_freqs.sort(key=lambda x: x[1], reverse=True)\n",
    "        if use_special_tokens:\n",
    "            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n",
    "            uniq_tokens = ['<pad>', '<bos>', '<eos>', '<unk>']\n",
    "        else:\n",
    "            self.unk, uniq_tokens = 0, ['<unk>']\n",
    "        uniq_tokens +=  [token for token, freq in self.token_freqs \n",
    "                         if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nmkyPk6oUTKd"
   },
   "outputs": [],
   "source": [
    "source = [[s.split(' ') for s in l] for l in data['dialogue_list'].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3HShTv-Z60Y2",
    "outputId": "ab567366-a1e2-4fea-a768-5179440503fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10013"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lAC69BTp9VF"
   },
   "outputs": [],
   "source": [
    "tmp_sourse = sum(source, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUM3IwRdtSUo"
   },
   "outputs": [],
   "source": [
    "tmp2_sourse = sum(tmp_sourse, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eHkJ3EhP7tpQ",
    "outputId": "f737d83b-debe-4835-b9c4-a9652fac07e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62563"
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tmp2_sourse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmReBnFS-ou0"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhrXYG5k91nh"
   },
   "outputs": [],
   "source": [
    "with open('./drive/My Drive/data_for_hw_9.pkl', 'wb') as f:\n",
    "    pickle.dump(tmp2_sourse, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ax0wdYU1-mnc"
   },
   "outputs": [],
   "source": [
    "#with open('./drive/My Drive/data_for_hw_9.pkl', 'rb') as f:\n",
    "with open('data_for_hw_9.pkl', 'rb') as f:\n",
    "    tmp2_sourse = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:49.114035Z",
     "start_time": "2019-12-01T16:33:49.108556Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CfGDDF-1mE8E",
    "outputId": "e5eb9fa8-d45d-4321-b5e7-2aa89403bbee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19439"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(tokens):\n",
    "    #tokens = [token for line in tokens for token in line]\n",
    "    return Vocab(tokens, min_freq=3, use_special_tokens=True)\n",
    "\n",
    "src_vocab = build_vocab(tmp2_sourse)\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "wXwXFEnoU2pn",
    "outputId": "fc813990-e911-418c-dd51-6a0492e4f94b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 4, 148, 38, 68],\n",
       " [18, 4, 287, 1384, 11989, 236, 737, 3817, 4],\n",
       " [23, 1439, 7, 31, 85, 2476, 4, 6, 29, 14, 189],\n",
       " [14, 79, 5, 93, 74, 5, 17, 11, 145, 151, 4, 6, 1260, 14],\n",
       " [8, 6, 1197, 3512, 4, 11, 802, 861, 11, 653, 655, 119, 22],\n",
       " [318, 13, 5, 152, 4],\n",
       " [13, 1073, 12, 7681, 15, 6406, 6519, 7, 6, 136, 21, 448, 4]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab[source[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:49.122605Z",
     "start_time": "2019-12-01T16:33:49.116449Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "colab_type": "code",
    "id": "PEjNcRCEmE8J",
    "outputId": "99351d62-0b9c-46dc-8d4e-a4f9dfd432f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 4,\n",
       " 148,\n",
       " 38,\n",
       " 68,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad(line, max_len, padding_token):\n",
    "    if len(line) > max_len:\n",
    "        return line[:max_len]\n",
    "    return line + [padding_token] * (max_len - len(line))\n",
    "\n",
    "pad(src_vocab[source[0][0]], 50, src_vocab.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujVpFBfnVp_D"
   },
   "outputs": [],
   "source": [
    "def pad_line(lines, max_len_line, padding_token):\n",
    "    if len(lines) > max_len_line:\n",
    "        return lines[:max_len_line]\n",
    "    return lines + [padding_token] * (max_len_line - len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:49.131148Z",
     "start_time": "2019-12-01T16:33:49.125106Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3NB9uSA-mE8M"
   },
   "outputs": [],
   "source": [
    "def build_array(lines, vocab, max_len, max_len_line):\n",
    "    lines = [vocab[line] for line in lines]\n",
    "    lines = [[pad([vocab.bos] + l + [vocab.eos],max_len,vocab.pad) for l in line] for line in lines]\n",
    "    lines = [pad_line(line,max_len_line,[vocab.pad]*max_len) for line in lines]\n",
    "    array = torch.LongTensor(lines)\n",
    "    valid_len = (array != vocab.pad).sum(axis=2)\n",
    "    mask = torch.zeros_like(array).type(torch.float)\n",
    "    mask = (array != vocab.pad).type(torch.float)\n",
    "    #for i in range(len(valid_len)):\n",
    "    #    mask[i, :valid_len[i]]=1.\n",
    "    return array, valid_len, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:49.138015Z",
     "start_time": "2019-12-01T16:33:49.133576Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0GknL2yqmE8P"
   },
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, max_len, max_len_lines): \n",
    "    src_vocab = build_vocab(tmp2_sourse)\n",
    "    src_array, src_valid_len, src_trg = build_array(source[:1000], src_vocab, max_len, max_len_lines)\n",
    "    train_data = torch.utils.data.TensorDataset(src_array, src_valid_len, src_trg)\n",
    "    train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "    return src_vocab, train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:49.182063Z",
     "start_time": "2019-12-01T16:33:49.175703Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hdKCPxyimE8U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.set_state = None\n",
    "\n",
    "    def forward(self, X, is_set_state, *args):\n",
    "        X = self.embedding(X) \n",
    "        X = X.permute(1, 0, 2)\n",
    "        if is_set_state and (self.set_state is not None): \n",
    "            \n",
    "            state = (self.set_state[0].detach(),self.set_state[1].detach())\n",
    "        else:\n",
    "            \n",
    "            state = (torch.zeros((self.num_layers, X.shape[1],self.num_hiddens)), torch.zeros((self.num_layers, X.shape[1],self.num_hiddens)))\n",
    "        out, self.set_state = self.rnn(X, state)\n",
    "        \n",
    "        return out, self.set_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:16:43.077500Z",
     "start_time": "2019-12-01T19:16:43.070765Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "e8j-NI44mE8_"
   },
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Module):  \n",
    "    def __init__(self, inputs, units, dropout, **kwargs):\n",
    "        super(MLPAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(inputs, units, bias=False)\n",
    "        self.W_q = nn.Linear(inputs, units, bias=False)\n",
    "        self.v = nn.Linear(units, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        query, key = torch.tanh(self.W_k(query)), torch.tanh(self.W_q(key))\n",
    "        # expand query to (batch_size, #querys, 1, units), and key to \n",
    "        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.  \n",
    "        features = query.unsqueeze(2) + key.unsqueeze(1)\n",
    "        scores = self.v(features).squeeze(dim=-1)\n",
    "        attention_weights = self.dropout(scores.softmax(dim=-1))\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:17:55.737695Z",
     "start_time": "2019-12-01T19:17:55.726847Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dSMDfrKYmE9F"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_cell = MLPAttention(num_hiddens, num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return (enc_outputs[0].permute(1,0,2), enc_outputs[1])\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, hidden_state = state\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs = []\n",
    "        for x in X:\n",
    "            # query shape: (batch_size, 1, hidden_size)\n",
    "            query = hidden_state[0][-1].unsqueeze(1)\n",
    "            # context has same shape as query\n",
    "            context = self.attention_cell(query, enc_outputs, enc_outputs)\n",
    "            # concatenate on the feature dimension\n",
    "            x = torch.cat([context, x.unsqueeze(1)], dim=-1)\n",
    "            # reshape x to (1, batch_size, embed_size+hidden_size)\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state]\n",
    "            \n",
    "            \n",
    "       # out, state = self.rnn(X, state)\n",
    "       # out = self.dense(out.view(-1, self.num_hiddens)).view(out.shape[0], out.shape[1], self.vocab_size).permute(1, 0, 2)\n",
    "       # return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, is_set_state, *args):\n",
    "        enc_outputs = self.encoder(enc_X, is_set_state, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:49.391439Z",
     "start_time": "2019-12-01T16:33:49.383164Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ru_v9db3mE8m"
   },
   "outputs": [],
   "source": [
    "def train(model, data_iter, lr, num_epochs, out_vocab_size,trainer=None):\n",
    "    if trainer is None:\n",
    "        trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    tic = time.time()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        l_sum, num_tokens_sum , num_batch= 0.0, 0.0, 0\n",
    "        print(f'batch ', end=' ')\n",
    "        for batch in data_iter:\n",
    "            print(f' {num_batch} ', end=' ')\n",
    "            #is_set_state = False\n",
    "            X, X_vlen, mask = [x for x in batch]\n",
    "            #X, X_vlen, Y, Y_vlen, mask = [x for x in batch]\n",
    "            use_state = False\n",
    "            for i in range(X.shape[1]-1):\n",
    "                trainer.zero_grad()\n",
    "                #Y_input, Y_label, Y_vlen, mask = Y[:,:-1], Y[:,1:], Y_vlen-1, mask[:,1:]\n",
    "                #print(X.shape, X_vlen.shape, mask.shape)     \n",
    "                cur_X, cur_X_vlen, cur_Y, cur_Y_vlen, cur_mask = X[:,i,:], X_vlen[:,i], X[:,i+1,:], X_vlen[:,i+1], mask[:,i+1,:]  \n",
    "                \n",
    "                cur_Y_input, cur_Y_label, cur_Y_vlen, cur_mask = cur_Y[:,:-1], cur_Y[:,1:], cur_Y_vlen-1, cur_mask[:,1:]\n",
    "                \n",
    "                cur_Y_hat, _ = model(cur_X, cur_Y_input, use_state, cur_X_vlen, cur_Y_vlen )\n",
    "            \n",
    "                l = (loss(cur_Y_hat.reshape(-1,out_vocab_size), cur_Y_label.reshape(-1)) * cur_mask.reshape(-1)).sum() \n",
    "            \n",
    "                l.backward()\n",
    "                num_tokens = cur_Y_vlen.sum().item()\n",
    "                trainer.step()\n",
    "                l_sum += l.item()\n",
    "                num_tokens_sum += num_tokens\n",
    "                use_state = True\n",
    "            num_batch+=1 \n",
    "        print(\"epoch %d, loss %.3f, time %.1f sec\" % (\n",
    "            epoch, l_sum/num_tokens_sum, time.time()-tic))\n",
    "        tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:21:38.863638Z",
     "start_time": "2019-12-01T19:18:40.387133Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "C0AWJhXomE9L",
    "outputId": "e4cc3466-8a24-4223-b18a-fb189f4b6082",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 1, loss 7.438, time 217.6 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 2, loss 6.949, time 217.0 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 3, loss 6.439, time 215.9 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 4, loss 6.095, time 216.2 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 5, loss 5.894, time 215.4 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 6, loss 5.741, time 216.8 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 7, loss 5.613, time 218.4 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 8, loss 5.496, time 215.9 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 9, loss 5.403, time 218.8 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 10, loss 5.316, time 217.1 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 11, loss 5.242, time 219.4 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "epoch 12, loss 5.177, time 227.2 sec\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c871d610a8a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-dab1af62af3c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_iter, lr, num_epochs, out_vocab_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_Y_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_Y_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcur_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_Y_vlen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\n",
    "batch_size, max_len, max_len_lines = 100, 50, 40\n",
    "lr, num_epochs = 0.01, 75\n",
    "\n",
    "src_vocab, train_iter = load_data_nmt(batch_size=100, max_len=50, max_len_lines = 40)\n",
    "encoder = Encoder(\n",
    "    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = AttentionDecoder(\n",
    "    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "train(model, train_iter, lr, num_epochs, len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ju-MauE0mE9R"
   },
   "outputs": [],
   "source": [
    "def translate(model, src_sentence, src_vocab, tgt_vocab, max_len):\n",
    "    res = []\n",
    "    use_state = False\n",
    "    for sen in  src_sentence:\n",
    "        src_tokens = src_vocab[sen.lower().split(' ')]\n",
    "        src_len = len(src_tokens)\n",
    "        if src_len < max_len:\n",
    "            src_tokens += [src_vocab.pad] * (max_len - src_len)\n",
    "        enc_X = torch.LongTensor(src_tokens)\n",
    "        enc_valid_length = torch.LongTensor([src_len])\n",
    "                    \n",
    "        enc_outputs = model.encoder(enc_X.unsqueeze(axis=0),use_state, enc_valid_length )\n",
    "        use_state = True\n",
    "        dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)\n",
    "        dec_X = torch.LongTensor([tgt_vocab.bos]).unsqueeze(axis=0)\n",
    "        predict_tokens = []\n",
    "        for _ in range(max_len):\n",
    "            Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "            dec_X = Y.argmax(axis=2)\n",
    "            py = dec_X.squeeze(axis=0).type(torch.long).item()\n",
    "            if py == tgt_vocab.eos:\n",
    "                break\n",
    "            predict_tokens.append(py)\n",
    "        res.append(f\"{sen} =>{' '.join(tgt_vocab.to_tokens(predict_tokens))}\")\n",
    "    return res  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:21:48.643911Z",
     "start_time": "2019-12-01T19:21:48.615104Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "_qqd9h-8mE9P",
    "outputId": "6ba0d1e4-de51-4f33-b083-b992bb3e67cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>я тоже люблю .', ' Как дела ? =>а я люблю читать .'] ['Привет . =>я тоже люблю .', 'Как ты себя чувствуешь ? =>я тоже люблю .', 'Что делаешь ? =>я тоже люблю .', 'Гулять пойдешь ? =>я тоже люблю .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "nnP3tRXDCbbh",
    "outputId": "47082755-b098-493d-ed5e-42a325c35b9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 5.356, time 221.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 5.107, time 218.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 4.974, time 218.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 4.901, time 219.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 4.818, time 217.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 4.753, time 218.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 4.698, time 218.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 4.653, time 217.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 4.611, time 218.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 4.571, time 220.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 4.543, time 220.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 4.508, time 222.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 4.481, time 225.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 4.454, time 226.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 4.425, time 228.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 16, loss 4.918, time 226.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 17, loss 4.547, time 223.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 18, loss 4.463, time 226.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 19, loss 4.422, time 228.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 20, loss 4.394, time 229.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 21, loss 4.370, time 230.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 22, loss 4.340, time 232.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 23, loss 4.313, time 233.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 24, loss 4.294, time 238.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 25, loss 4.270, time 240.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 26, loss 4.252, time 241.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 27, loss 4.237, time 242.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 28, loss 4.223, time 244.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 29, loss 4.208, time 244.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 30, loss 4.202, time 245.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 31, loss 4.190, time 248.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 32, loss 4.218, time 248.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 33, loss 4.222, time 249.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 34, loss 4.193, time 250.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 35, loss 4.177, time 249.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 36, loss 4.194, time 247.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 37, loss 4.181, time 247.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 38, loss 4.152, time 249.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 39, loss 4.137, time 250.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 40, loss 4.123, time 250.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 41, loss 4.101, time 250.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 42, loss 4.089, time 251.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 43, loss 4.080, time 251.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 44, loss 4.091, time 252.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 45, loss 4.108, time 252.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 46, loss 4.115, time 252.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 47, loss 4.113, time 252.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 48, loss 4.096, time 252.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 49, loss 4.096, time 252.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 50, loss 4.069, time 252.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 51, loss 4.044, time 253.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 52, loss 4.035, time 253.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 53, loss 4.022, time 253.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 54, loss 4.008, time 253.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 55, loss 4.000, time 254.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 56, loss 3.990, time 254.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 57, loss 3.983, time 255.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 58, loss 3.975, time 256.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 59, loss 3.963, time 256.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 60, loss 3.956, time 255.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 61, loss 3.950, time 254.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 62, loss 3.950, time 256.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 63, loss 3.944, time 255.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 64, loss 3.936, time 255.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 65, loss 3.929, time 255.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 66, loss 3.925, time 255.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 67, loss 3.923, time 255.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 68, loss 3.914, time 255.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 69, loss 3.909, time 255.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 70, loss 3.905, time 255.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 71, loss 3.905, time 257.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 72, loss 3.901, time 257.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 73, loss 3.893, time 255.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 74, loss 3.891, time 256.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 75, loss 3.998, time 257.0 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, lr, num_epochs, len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>привет . как дела ?', ' Как дела ? =>привет . я инженер .'] ['Привет . =>привет . как дела ?', 'Как ты себя чувствуешь ? =>привет . как дела ?', 'Что делаешь ? =>привет . как дела ?', 'Гулять пойдешь ? =>а я люблю готовить .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.983, time 256.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.897, time 256.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.892, time 257.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.885, time 256.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.877, time 256.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.877, time 256.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.868, time 256.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.860, time 256.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.860, time 256.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.855, time 256.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.853, time 256.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.847, time 255.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.847, time 255.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.839, time 255.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.830, time 254.8 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, lr, num_epochs, len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>привет . как дела ?', ' Как дела ? =>я тоже .'] ['Привет . =>привет . как дела ?', 'Как ты себя чувствуешь ? =>привет . как дела ?', 'Что делаешь ? =>я тоже .', 'Гулять пойдешь ? =>я тоже .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.916, time 254.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.855, time 255.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.848, time 254.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.842, time 254.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.842, time 254.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.834, time 254.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.832, time 254.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.825, time 254.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.823, time 254.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.810, time 253.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.825, time 254.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.816, time 253.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.814, time 253.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.821, time 253.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.821, time 253.9 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, lr, num_epochs, len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>как тебя зовут ?', ' Как дела ? =>привет !'] ['Привет . =>как тебя зовут ?', 'Как ты себя чувствуешь ? =>привет !', 'Что делаешь ? =>привет .', 'Гулять пойдешь ? =>я тоже .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.896, time 254.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.847, time 254.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.847, time 254.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.850, time 254.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.847, time 253.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.840, time 254.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.830, time 254.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.819, time 254.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.812, time 253.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.814, time 254.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.828, time 253.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.814, time 252.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.796, time 252.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.786, time 253.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.790, time 253.7 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, lr, num_epochs, len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>как дела . я тоже .', ' Как дела ? =>я тоже люблю . а вы ?'] ['Привет . =>как дела . я тоже .', 'Как ты себя чувствуешь ? =>как дела ?', 'Что делаешь ? =>как дела ?', 'Гулять пойдешь ? =>я тоже люблю . а вы ?']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.838, time 253.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.758, time 252.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.727, time 253.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.727, time 253.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.701, time 252.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.701, time 252.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.687, time 252.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.678, time 252.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.675, time 251.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.670, time 253.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.670, time 252.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.668, time 254.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.657, time 254.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.653, time 261.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.649, time 254.2 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, lr*0.7, num_epochs, len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>привет . как дела ?', ' Как дела ? =>привет . я работаю в автосервисе .'] ['Привет . =>привет . как дела ?', 'Как ты себя чувствуешь ? =>привет . как дела ?', 'Что делаешь ? =>я тоже .', 'Гулять пойдешь ? =>я тоже .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.704, time 250.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.659, time 250.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.657, time 250.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.649, time 251.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.646, time 248.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.641, time 247.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.639, time 248.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.632, time 248.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.635, time 249.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.628, time 246.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.622, time 244.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.618, time 240.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.624, time 248.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.621, time 255.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.621, time 253.2 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, lr*0.7, num_epochs, len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>как дела ?', ' Как дела ? =>привет . я работаю в офисе .'] ['Привет . =>как дела ?', 'Как ты себя чувствуешь ? =>как дела ?', 'Что делаешь ? =>как дела ?', 'Гулять пойдешь ? =>а я люблю готовить .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.825, time 242.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.774, time 237.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.771, time 240.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.748, time 244.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.708, time 238.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.679, time 243.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.677, time 248.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.694, time 246.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.670, time 247.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.654, time 259.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.666, time 267.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.674, time 262.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.657, time 251.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.661, time 244.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.648, time 248.1 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "train(model, train_iter, lr, num_epochs, len(src_vocab),trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>привет . как дела ?', ' Как дела ? =>привет . а ты ?'] ['Привет . =>привет . как дела ?', 'Как ты себя чувствуешь ? =>как дела ?', 'Что делаешь ? =>как дела ?', 'Гулять пойдешь ? =>я курьер .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.550, time 253.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.480, time 258.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.462, time 255.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.451, time 253.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.444, time 255.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.438, time 262.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.433, time 263.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.429, time 263.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.426, time 258.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.423, time 264.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.421, time 261.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.420, time 263.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.418, time 261.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.417, time 261.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.415, time 264.9 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.005\n",
    "trainer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "train(model, train_iter, lr, num_epochs, len(src_vocab), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>привет . как дела ?', ' Как дела ? =>привет . а я охотник , а ты ?'] ['Привет . =>привет . как дела ?', 'Как ты себя чувствуешь ? =>привет . как дела ?', 'Что делаешь ? =>привет . как дела ?', 'Гулять пойдешь ? =>я тоже .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.421, time 245.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.416, time 245.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.414, time 246.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.413, time 255.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.412, time 259.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.410, time 255.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.409, time 253.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.408, time 257.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.408, time 265.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.407, time 264.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.407, time 259.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.406, time 258.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.407, time 256.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.405, time 253.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.405, time 258.7 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "trainer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "train(model, train_iter, lr, num_epochs, len(src_vocab), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>привет . как дела ?', ' Как дела ? =>привет . а я охотник , а ты ?'] ['Привет . =>привет . как дела ?', 'Как ты себя чувствуешь ? =>привет . как дела ?', 'Что делаешь ? =>привет . как дела ?', 'Гулять пойдешь ? =>я тоже .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.389, time 254.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.386, time 253.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.385, time 257.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.384, time 252.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.383, time 251.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.383, time 255.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.382, time 256.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.382, time 258.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.382, time 260.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.381, time 261.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.381, time 259.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.381, time 263.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.380, time 264.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.380, time 262.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.380, time 254.4 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.00005\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "train(model, train_iter, lr, num_epochs, len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>привет . как дела ?', ' Как дела ? =>привет . я работаю в офисе .'] ['Привет . =>привет . как дела ?', 'Как ты себя чувствуешь ? =>как дела ?', 'Что делаешь ? =>как дела ?', 'Гулять пойдешь ? =>я тоже .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.379, time 261.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.379, time 264.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.379, time 262.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.379, time 259.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.379, time 250.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.379, time 250.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.379, time 242.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.379, time 241.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.379, time 240.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.379, time 241.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.379, time 240.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.379, time 240.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.379, time 240.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.379, time 240.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.379, time 240.3 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "trainer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "train(model, train_iter, lr, num_epochs, len(src_vocab), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет . =>привет . как дела ?', ' Как дела ? =>привет . я работаю в офисе .'] ['Привет . =>привет . как дела ?', 'Как ты себя чувствуешь ? =>как дела ?', 'Что делаешь ? =>как дела ?', 'Гулять пойдешь ? =>я тоже .']\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , ' Как дела ?'],['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "print(*all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 1, loss 3.378, time 240.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 2, loss 3.378, time 240.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 3, loss 3.378, time 240.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 4, loss 3.378, time 240.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 5, loss 3.378, time 239.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 6, loss 3.378, time 240.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 7, loss 3.378, time 240.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 8, loss 3.378, time 240.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 9, loss 3.378, time 240.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 10, loss 3.378, time 241.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 11, loss 3.378, time 240.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 12, loss 3.378, time 240.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 13, loss 3.378, time 239.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 14, loss 3.378, time 240.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 15, loss 3.378, time 240.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 16, loss 3.378, time 240.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 17, loss 3.378, time 240.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 18, loss 3.378, time 239.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 19, loss 3.378, time 241.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 20, loss 3.378, time 240.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 21, loss 3.378, time 240.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 22, loss 3.378, time 240.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 23, loss 3.378, time 240.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 24, loss 3.378, time 240.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 25, loss 3.378, time 240.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 26, loss 3.378, time 240.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 27, loss 3.378, time 240.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 28, loss 3.378, time 239.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 29, loss 3.378, time 240.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 30, loss 3.378, time 239.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 31, loss 3.378, time 240.3 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 32, loss 3.378, time 241.0 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 33, loss 3.378, time 240.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 34, loss 3.378, time 240.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 35, loss 3.378, time 240.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 36, loss 3.378, time 240.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 37, loss 3.378, time 240.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 38, loss 3.378, time 240.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 39, loss 3.378, time 240.5 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 40, loss 3.378, time 240.8 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 41, loss 3.378, time 240.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 42, loss 3.378, time 240.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 43, loss 3.378, time 240.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 44, loss 3.378, time 240.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 45, loss 3.378, time 240.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 46, loss 3.378, time 240.7 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 47, loss 3.378, time 239.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 48, loss 3.378, time 240.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 49, loss 3.378, time 240.9 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 50, loss 3.378, time 240.2 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 51, loss 3.378, time 240.4 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 52, loss 3.378, time 240.6 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 53, loss 3.378, time 240.1 sec\n",
      "batch   0   1   2   3   4   5   6   7   8   9  epoch 54, loss 3.378, time 240.0 sec\n",
      "batch   0   1   2  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-17b0d0de89fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.000005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-8e7e74ff599e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_iter, lr, num_epochs, out_vocab_size, trainer)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_Y_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_Y_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcur_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_Y_vlen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.000005\n",
    "trainer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "train(model, train_iter, lr, 100, len(src_vocab), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "1. Привет . =>привет . как дела ?\n",
      "2. Как дела ? =>привет . а я охотник , а ты ?\n",
      "----------\n",
      "1. Привет . =>привет . как дела ?\n",
      "2. Как ты себя чувствуешь ? =>как дела ?\n",
      "3. Что делаешь ? =>привет . как дела ?\n",
      "4. Гулять пойдешь ? =>я тоже .\n",
      "----------\n",
      "1. Здравствуй . Как тебя зовут ? =>здравствуйте . как дела ?\n",
      "2. А сколько тебе лет ? =>а я стригу .\n",
      "3. Я работаю сталеваром . А ты ? =>я певец .\n",
      "4. Я люблю смотреть кино ? =>привет . а я работаю в автосервисе .\n",
      "5. Что ты сейчас делаешь ? =>привет .\n",
      "6. Где ты работаешь ? =>привет .\n",
      "7. Пока . =>привет . а ты ?\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "for sentence in [['Привет .' , 'Как дела ?'],\n",
    "                 ['Привет .','Как ты себя чувствуешь ?', 'Что делаешь ?','Гулять пойдешь ?'],\n",
    "                 ['Здравствуй . Как тебя зовут ?','А сколько тебе лет ?','Я работаю сталеваром . А ты ?','Я люблю смотреть кино ?','Что ты сейчас делаешь ?','Где ты работаешь ?','Пока .']]:\n",
    "    all_sent.append( translate(model, sentence, src_vocab, src_vocab, max_len)) \n",
    "for i in all_sent:\n",
    "    print('----------')\n",
    "    for ind, s in enumerate(i):\n",
    "        print(f\"{ind+1}. {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество решения так себе. \n",
    "Не получилось на данной архитектуре получить решение лучше.\n",
    "Долго обучал модель,используя разные оптимизаторы, \n",
    "но помогло не очень. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw_9.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
